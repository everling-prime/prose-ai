{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T22:40:23.448248Z",
     "start_time": "2017-09-12T22:40:23.437329Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Quill stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy and TextaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:29:52.339300Z",
     "start_time": "2017-09-12T19:29:52.333841Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_path = 'corpus_data/comp_ling.txt'\n",
    "\n",
    "with open(doc_path) as f:\n",
    "    input_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:29:57.738200Z",
     "start_time": "2017-09-12T19:29:52.488841Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T04:27:29.186949Z",
     "start_time": "2017-09-13T04:27:29.182580Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T04:27:30.860684Z",
     "start_time": "2017-09-13T04:27:30.854093Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_doc = '''\n",
    "# Oxygen is a chemical element with symbol O and atomic number 8. A member of the chalcogen group on the periodic table, it is a highly reactive nonmetal and oxidizing agent that forms oxides with most elements as well as other compounds.\n",
    "# '''\n",
    "\n",
    "def to_spacy_doc(raw_doc):\n",
    "    '''Converts a raw string into a spaCy document'''\n",
    "    return nlp(raw_doc)\n",
    "\n",
    "def to_textacy_doc(raw_doc):\n",
    "    '''Converts a raw string into a spaCy doc, then a textacy doc'''\n",
    "    if isinstance(to_spacy_doc(\"test\"), spacy.tokens.doc.Doc):\n",
    "        return textacy.Doc(raw_doc)\n",
    "    else:\n",
    "        return textacy.Doc(nlp(raw_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T02:08:10.465640Z",
     "start_time": "2017-09-09T02:08:10.429246Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Today, \n",
       " \n",
       " The Association for Computational Linguistics, \n",
       " \n",
       " ..., 1, 2, 2.1, 2.2, 2.3, 2.3.1, 2.3.2, \n",
       " 2.4, 3, 4, 5, 6, 7\tReferences, 8\tExternal, United States, 1950s, Russian, 1960s, one, one, one, four, one, Crucially, Pólya, modern-day, English, One, One, English, IBM, over 4.5 million words, American, English, One, two, early in the lifetime of a field of study, English, Japanese, Japanese, Japanese, October 2015, only half, Alan Turing, Turing Test, 1950, Alan Turing, one day, two, one, Turing, Today, Turing, Joseph Weizenbaum, MIT, ELIZA, One, ELIZA, Joseph Weizenbaum, MIT, 1966, Rogerian, ELIZA, ELIZA, ELIZA, first, first, Markov, translation.[25] The, German, French, first, five, ELIZA, Siri, Bayesian, Bledsoe, Browing, 1959, one, Bayesian, Mosteller, Wallace, 1963, Federalist Papers, Madison, 1971, Terry Winograd, SHRDLU, SHRDLU, NASA, LUNAR, Apollo, 1960s and 1970s, Markov, Rabiner, 1989.[32, late 70s, IBM, Bayesian, Apple, Siri, Google Translate, Social, Twitter, four, four]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_named_entities(doc):\n",
    "    nes = textacy.extract.named_entities(doc)\n",
    "    return [ne for ne in nes]\n",
    "\n",
    "get_named_entities(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T02:14:10.699235Z",
     "start_time": "2017-09-09T02:14:10.696096Z"
    }
   },
   "outputs": [],
   "source": [
    "if not isinstance(doc, textacy.doc.Doc):\n",
    "        print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'automated_readability_index': 22.352141689526015,\n",
       " 'coleman_liau_index': 16.682927688311693,\n",
       " 'flesch_kincaid_grade_level': 19.052077766395673,\n",
       " 'flesch_readability_ease': 19.26507900874003,\n",
       " 'gulpease_index': 41.068861371186955,\n",
       " 'gunning_fog_index': 22.472696136278014,\n",
       " 'lix': 72.24939361765064,\n",
       " 'smog_index': 18.80095838887095,\n",
       " 'wiener_sachtextformel': 12.265282847241465}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_readability_stats(textacy_doc):\n",
    "    ts = text_stats.TextStats(textacy_doc)\n",
    "    return ts.readability_stats\n",
    "\n",
    "get_readability_stats(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', 0.02416255077377969),\n",
       " ('linguistic', 0.02043304879128454),\n",
       " ('computational', 0.01746222784332213),\n",
       " ('human', 0.011689631474031183),\n",
       " ('computer', 0.011480717062340147),\n",
       " ('model', 0.011188761077018177),\n",
       " ('word', 0.011081683346972649),\n",
       " ('approach', 0.008327125663422037),\n",
       " ('speech', 0.007841891597153119),\n",
       " ('program', 0.007792902818308204)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.keyterms.key_terms_from_semantic_network(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T02:20:01.789808Z",
     "start_time": "2017-09-09T02:20:01.766571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'linguistic',\n",
       " 'computational',\n",
       " 'human',\n",
       " 'computer',\n",
       " 'model',\n",
       " 'word',\n",
       " 'approach',\n",
       " 'speech',\n",
       " 'program']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_textacy_key_terms(doc):\n",
    "    if not isinstance(doc, textacy.doc.Doc):\n",
    "        doc = to_textacy_doc(doc)\n",
    "    term_prob_pairs = textacy.keyterms.key_terms_from_semantic_network(doc)\n",
    "    terms = [term[0] for term in term_prob_pairs]\n",
    "    return terms\n",
    "get_textacy_key_terms(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get top 5 textacy key_terms and generate Empath category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T06:29:13.209527Z",
     "start_time": "2017-09-13T06:29:12.508896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(2 docs; 10783 tokens)\n",
      "Most burger shops have also adopted a pizzeria - like shape , cut from whole potatoes using a specialized spiral slicer . \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'90% consumed by the early 1970s .'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import markovify\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# corpus_path = \"/corpus_data/comp_ling.txt\"\n",
    "# corpus_name = \"comp_ling\"\n",
    "corpus = textacy.Corpus('en', [wiki_blob, \" \"])\n",
    "#corpus = textacy.Corpus.add_doc(textacy.Corpus('en'), to_textacy_doc(wiki_blob))\n",
    "#corpus = wiki_blob_spacy\n",
    "print(corpus)\n",
    "\n",
    "class TaggedText(markovify.Text):\n",
    "\n",
    "    def sentence_split(self, text):\n",
    "        \"\"\"\n",
    "        Splits full-text string into a list of sentences.\n",
    "        \"\"\"\n",
    "        sentence_list = []\n",
    "        for doc in corpus:\n",
    "            sentence_list += list(doc.sents)\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    def word_split(self, sentence):\n",
    "        \"\"\"\n",
    "        Splits a sentence into a list of words.\n",
    "        \"\"\"\n",
    "        #print(sentence)\n",
    "        return [\"::\".join((word.orth_,word.pos_)) for word in sentence]\n",
    "\n",
    "    def word_join(self, words):\n",
    "        sentence = \" \".join(word.split(\"::\")[0] for word in words)\n",
    "        return sentence\n",
    "\n",
    "    def test_sentence_input(self, sentence):\n",
    "        \"\"\"\n",
    "        A basic sentence filter. This one rejects sentences that contain\n",
    "        the type of punctuation that would look strange on its own\n",
    "        in a randomly-generated sentence. \n",
    "        \"\"\"\n",
    "        sentence = sentence.text\n",
    "        reject_pat = re.compile(r\"(^')|('$)|\\s'|'\\s|[\\\"(\\(\\)\\[\\])]\")\n",
    "        # Decode unicode, mainly to normalize fancy quotation marks\n",
    "        if sentence.__class__.__name__ == \"str\":\n",
    "            decoded = sentence\n",
    "        else:\n",
    "            decoded = unidecode(sentence)\n",
    "        # Sentence shouldn't contain problematic characters\n",
    "        if re.search(reject_pat, decoded): return False\n",
    "        return True\n",
    "\n",
    "    def generate_corpus(self, text):\n",
    "        \"\"\"\n",
    "        Given a text string, returns a list of lists; that is, a list of\n",
    "        \"sentences,\" each of which is a list of words. Before splitting into \n",
    "        words, the sentences are filtered through `self.test_sentence_input`\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_split(text)\n",
    "        passing = filter(self.test_sentence_input, sentences)\n",
    "        runs = list(map(self.word_split, passing))\n",
    "        #print(runs[:10])\n",
    "        return runs\n",
    "\n",
    "# Generated the model\n",
    "model = TaggedText(corpus)\n",
    "# A sentence based on the model\n",
    "print(model.make_sentence())\n",
    "model.make_short_sentence(max_chars=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computational</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>-4.329765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an</td>\n",
       "      <td>-5.953294</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interdisciplinary</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>field</td>\n",
       "      <td>-9.710699</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>concerned</td>\n",
       "      <td>-9.861534</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>with</td>\n",
       "      <td>-5.363765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>statistical</td>\n",
       "      <td>-11.639928</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>or</td>\n",
       "      <td>-5.715355</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rule</td>\n",
       "      <td>-9.545105</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.202416</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>based</td>\n",
       "      <td>-8.318480</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>modeling</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>of</td>\n",
       "      <td>-4.128464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>natural</td>\n",
       "      <td>-9.312656</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>language</td>\n",
       "      <td>-8.811478</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>from</td>\n",
       "      <td>-6.028811</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.983075</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>computational</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>perspective</td>\n",
       "      <td>-9.924529</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>as</td>\n",
       "      <td>-5.507394</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>well</td>\n",
       "      <td>-7.117937</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>as</td>\n",
       "      <td>-5.507394</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>study</td>\n",
       "      <td>-9.603720</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>of</td>\n",
       "      <td>-4.128464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>appropriate</td>\n",
       "      <td>-10.032200</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>pictures</td>\n",
       "      <td>-9.748672</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>of</td>\n",
       "      <td>-4.128464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.983075</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>red</td>\n",
       "      <td>-9.453293</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>truck</td>\n",
       "      <td>-10.898046</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>search</td>\n",
       "      <td>-9.428295</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>engine</td>\n",
       "      <td>-10.291152</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>will</td>\n",
       "      <td>-6.105684</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>still</td>\n",
       "      <td>-7.043760</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>find</td>\n",
       "      <td>-7.562676</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>information</td>\n",
       "      <td>-8.817930</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>desired</td>\n",
       "      <td>-11.827469</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>by</td>\n",
       "      <td>-6.114920</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>matching</td>\n",
       "      <td>-11.781653</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>words</td>\n",
       "      <td>-8.599854</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>such</td>\n",
       "      <td>-7.619059</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>as</td>\n",
       "      <td>-5.507394</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>\"</td>\n",
       "      <td>-4.700859</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>four</td>\n",
       "      <td>-9.416868</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.202416</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>wheeled</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>\"</td>\n",
       "      <td>-4.700859</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>with</td>\n",
       "      <td>-5.363765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>\"</td>\n",
       "      <td>-4.700859</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>car\".[37</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>]</td>\n",
       "      <td>-5.498423</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>-4.458347</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3756 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  log_probability stop? punctuation? whitespace?  \\\n",
       "0         Computational       -19.579313                                  \n",
       "1           linguistics       -19.579313                                  \n",
       "2                    is        -4.329765   Yes                            \n",
       "3                    an        -5.953294   Yes                            \n",
       "4     interdisciplinary       -19.579313                                  \n",
       "5                 field        -9.710699                                  \n",
       "6             concerned        -9.861534                                  \n",
       "7                  with        -5.363765   Yes                            \n",
       "8                   the        -3.425446   Yes                            \n",
       "9           statistical       -11.639928                                  \n",
       "10                   or        -5.715355   Yes                            \n",
       "11                 rule        -9.545105                                  \n",
       "12                    -        -5.202416                Yes               \n",
       "13                based        -8.318480                                  \n",
       "14             modeling       -19.579313                                  \n",
       "15                   of        -4.128464   Yes                            \n",
       "16              natural        -9.312656                                  \n",
       "17             language        -8.811478                                  \n",
       "18                 from        -6.028811   Yes                            \n",
       "19                    a        -3.983075   Yes                            \n",
       "20        computational       -19.579313                                  \n",
       "21          perspective        -9.924529                                  \n",
       "22                    ,        -3.391480                Yes               \n",
       "23                   as        -5.507394   Yes                            \n",
       "24                 well        -7.117937   Yes                            \n",
       "25                   as        -5.507394   Yes                            \n",
       "26                  the        -3.425446   Yes                            \n",
       "27                study        -9.603720                                  \n",
       "28                   of        -4.128464   Yes                            \n",
       "29          appropriate       -10.032200                                  \n",
       "...                 ...              ...   ...          ...         ...   \n",
       "3726           pictures        -9.748672                                  \n",
       "3727                 of        -4.128464   Yes                            \n",
       "3728                  a        -3.983075   Yes                            \n",
       "3729                red        -9.453293                                  \n",
       "3730              truck       -10.898046                                  \n",
       "3731                  ,        -3.391480                Yes               \n",
       "3732                the        -3.425446   Yes                            \n",
       "3733             search        -9.428295                                  \n",
       "3734             engine       -10.291152                                  \n",
       "3735               will        -6.105684   Yes                            \n",
       "3736              still        -7.043760   Yes                            \n",
       "3737               find        -7.562676                                  \n",
       "3738                the        -3.425446   Yes                            \n",
       "3739        information        -8.817930                                  \n",
       "3740            desired       -11.827469                                  \n",
       "3741                 by        -6.114920   Yes                            \n",
       "3742           matching       -11.781653                                  \n",
       "3743              words        -8.599854                                  \n",
       "3744               such        -7.619059   Yes                            \n",
       "3745                 as        -5.507394   Yes                            \n",
       "3746                  \"        -4.700859                Yes               \n",
       "3747               four        -9.416868   Yes                            \n",
       "3748                  -        -5.202416                Yes               \n",
       "3749            wheeled       -19.579313                                  \n",
       "3750                  \"        -4.700859                Yes               \n",
       "3751               with        -5.363765   Yes                            \n",
       "3752                  \"        -4.700859                Yes               \n",
       "3753           car\".[37       -19.579313                                  \n",
       "3754                  ]        -5.498423                Yes               \n",
       "3755               \\n\\n        -4.458347                            Yes   \n",
       "\n",
       "     number? out of vocab.?  \n",
       "0                            \n",
       "1                            \n",
       "2                            \n",
       "3                            \n",
       "4                            \n",
       "5                            \n",
       "6                            \n",
       "7                            \n",
       "8                            \n",
       "9                            \n",
       "10                           \n",
       "11                           \n",
       "12                           \n",
       "13                           \n",
       "14                           \n",
       "15                           \n",
       "16                           \n",
       "17                           \n",
       "18                           \n",
       "19                           \n",
       "20                           \n",
       "21                           \n",
       "22                           \n",
       "23                           \n",
       "24                           \n",
       "25                           \n",
       "26                           \n",
       "27                           \n",
       "28                           \n",
       "29                           \n",
       "...      ...            ...  \n",
       "3726                         \n",
       "3727                         \n",
       "3728                         \n",
       "3729                         \n",
       "3730                         \n",
       "3731                         \n",
       "3732                         \n",
       "3733                         \n",
       "3734                         \n",
       "3735                         \n",
       "3736                         \n",
       "3737                         \n",
       "3738                         \n",
       "3739                         \n",
       "3740                         \n",
       "3741                         \n",
       "3742                         \n",
       "3743                         \n",
       "3744                         \n",
       "3745                         \n",
       "3746                         \n",
       "3747     Yes                 \n",
       "3748                         \n",
       "3749                         \n",
       "3750                         \n",
       "3751                         \n",
       "3752                         \n",
       "3753                    Yes  \n",
       "3754                         \n",
       "3755                         \n",
       "\n",
       "[3756 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in doc]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=10):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in food2vec.most_similar(positive=[token], topn=topn):\n",
    "\n",
    "        print u'{:20} {}'.format(word, round(similarity, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:58:27.722628Z",
     "start_time": "2017-09-12T19:58:27.712079Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Wikipedia wrapper (API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T22:11:34.826648Z",
     "start_time": "2017-09-12T22:11:34.798159Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from wikipedia import DisambiguationError, PageError, RedirectError\n",
    "\n",
    "def get_wiki_page(search_string, summary=False, content=False):\n",
    "    '''Returns results from searching for search_string with wikipedia wrapper library. \n",
    "       Note: Makes a web request'''\n",
    "    try:\n",
    "        page = wikipedia.page(search_string)\n",
    "        page_data = {\"url\":page.url,\n",
    "                     \"title\":page.title}\n",
    "        if content:\n",
    "            page_data[\"content\"] = page.content # Full text content of page.\n",
    "        if summary:\n",
    "            page_data[\"summary\"] = page.summary # Summary section only.\n",
    "    \n",
    "    except DisambiguationError as e:\n",
    "        return get_wiki_page(e.options[0]) #naively choose first option\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "    return page_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T08:31:43.099509Z",
     "start_time": "2017-09-09T08:31:43.096341Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_doc = \"London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T22:09:32.718322Z",
     "start_time": "2017-09-12T22:09:32.705696Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    '''Given a text document, extracts named entities using spaCy and builds a dict of metadata for each.\n",
    "    \n",
    "    Example Entity Type Labels:\n",
    "    ORGANIZATION\tGeorgia-Pacific Corp., WHO\n",
    "    PERSON\tEddy Bonte, President Obama\n",
    "    LOCATION\tMurray River, Mount Everest\n",
    "    DATE\tJune, 2008-06-29\n",
    "    TIME\ttwo fifty a m, 1:30 p.m.\n",
    "    MONEY\t175 million Canadian Dollars, GBP 10.40\n",
    "    PERCENT\ttwenty pct, 18.75 %\n",
    "    FACILITY\tWashington Monument, Stonehenge\n",
    "    GPE\tSouth East Asia, Midlothian\n",
    "    '''\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    named_entities = defaultdict(dict)\n",
    "    for ent in doc.ents:\n",
    "        ent_name = ent.text\n",
    "        named_entities[ent_name]['label'] = ent.label_\n",
    "        named_entities[ent_name]['text'] = ent.text\n",
    "        wiki_url = None #get_wiki_page(str(ent))['url']\n",
    "        if wiki_url:\n",
    "            named_entities[ent_name]['url'] = wiki_url\n",
    "        \n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T05:23:31.457864Z",
     "start_time": "2017-09-09T05:23:31.445282Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<a href=\"www.google.com\" class=link_class title=\"Custom Title\">Google</a>'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bleach\n",
    "\n",
    "def set_link_title(attrs, new=False):\n",
    "    attrs[(None, u'title')] = u'AI-provided Link'\n",
    "    return attrs\n",
    "\n",
    "def linkify(string):\n",
    "    '''Calls bleach.linkify.\n",
    "    Converts urls in the input string into links. \n",
    "    Returns a string of HTML.'''\n",
    "    if type(string) != str:\n",
    "        raise TypeError(\"input should be a string\")\n",
    "        \n",
    "    linker = Linker(callbacks=[set_link_title])\n",
    "    return bleach.linkify(string)\n",
    "\n",
    "def create_hyperlink(url, display_text, attrs=\"\"):\n",
    "    '''Optional attrs is a string of tag attributes.\n",
    "       Example call:\n",
    "       create_hyperlink('www.google.com', 'Google', \n",
    "       ... attrs = 'class=link_class title=\"Custom Title\"')'''\n",
    "    hyperlink_format = '<a href=\"{link}\" {attrs}>{text}</a>'\n",
    "    return hyperlink_format.format(link=url, attrs=attrs, text=display_text)\n",
    "\n",
    "create_hyperlink('www.google.com', 'Google', attrs='class=link_class title=\"Custom Title\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T22:16:13.150968Z",
     "start_time": "2017-09-12T22:16:13.138823Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### HTML-related functions\n",
    "def create_hyperlink(url, display_text, attrs=\"\"):\n",
    "    '''Optional attrs is a string of tag attributes.\n",
    "       \n",
    "       create_hyperlink('www.google.com', 'Google', \n",
    "       ... attrs = 'class=link_class title=\"Custom Title\"')'''\n",
    "    \n",
    "    hyperlink_format = '<a href=\"{link}\" {attrs}>{text}</a>'\n",
    "    return hyperlink_format.format(link=url, attrs=attrs, text=display_text)\n",
    "\n",
    "def linkify_entity(ent_dict):\n",
    "    '''Operates on extracted named entities. Returns HTML string.'''\n",
    "    ent_type = ent_dict['label']\n",
    "    text = ent_dict['text'] \n",
    "    url = get_wiki_page(text)['url']\n",
    "    attrs = 'class=\"{ent_type}\" title=\"{text}\"'.format(ent_type=ent_type, text=text)\n",
    "    return create_hyperlink(url, text, attrs=attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T22:16:13.545874Z",
     "start_time": "2017-09-12T22:16:13.309231Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'GPE', 'text': 'London'}\n",
      "<a href=\"https://en.wikipedia.org/wiki/London\" class=\"GPE\" title=\"London\">London</a>\n"
     ]
    }
   ],
   "source": [
    "test_doc = \"London is a city in the U.K.\"\n",
    "london = extract_named_entities(test_doc)[\"London\"]\n",
    "print(london)\n",
    "print(linkify_entity(london))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia XML Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T06:45:16.997470Z",
     "start_time": "2017-09-13T06:45:16.994459Z"
    }
   },
   "source": [
    "#### http://www.heatonresearch.com/2017/03/03/python-basic-wikipedia-parsing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T06:45:53.940022Z",
     "start_time": "2017-09-13T06:45:53.934493Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "PATH_WIKI_XML = '/Users/davideverling/Projects/Data'\n",
    "FILENAME_WIKI = 'enwiki-latest-pages-articles.xml'\n",
    "FILENAME_ARTICLES = 'articles.csv'\n",
    "FILENAME_REDIRECT = 'articles_redirect.csv'\n",
    "FILENAME_TEMPLATE = 'articles_template.csv'\n",
    "ENCODING = \"utf-8\"\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "def strip_tag_name(t):\n",
    "    t = elem.tag\n",
    "    idx = k = t.rfind(\"}\")\n",
    "    if idx != -1:\n",
    "        t = t[idx + 1:]\n",
    "    return t\n",
    "\n",
    "pathWikiXML = os.path.join(PATH_WIKI_XML, FILENAME_WIKI)\n",
    "pathArticles = os.path.join(PATH_WIKI_XML, FILENAME_ARTICLES)\n",
    "pathArticlesRedirect = os.path.join(PATH_WIKI_XML, FILENAME_REDIRECT)\n",
    "pathTemplateRedirect = os.path.join(PATH_WIKI_XML, FILENAME_TEMPLATE)\n",
    "\n",
    "totalCount = 0\n",
    "articleCount = 0\n",
    "redirectCount = 0\n",
    "templateCount = 0\n",
    "title = None\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-13T06:47:48.296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n",
      "100,000\n"
     ]
    }
   ],
   "source": [
    "'''Begin streaming the XML file and \n",
    "write the headers for the 3 CSV files that will be built according to the data found in the XML.'''\n",
    "\n",
    "with codecs.open(pathArticles, \"w\", ENCODING) as articlesFH, \\\n",
    "        codecs.open(pathArticlesRedirect, \"w\", ENCODING) as redirectFH, \\\n",
    "        codecs.open(pathTemplateRedirect, \"w\", ENCODING) as templateFH:\n",
    "    articlesWriter = csv.writer(articlesFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    redirectWriter = csv.writer(redirectFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    templateWriter = csv.writer(templateFH, quoting=csv.QUOTE_MINIMAL)\n",
    "    articlesWriter.writerow(['id', 'title', 'redirect'])\n",
    "    redirectWriter.writerow(['id', 'title', 'redirect'])\n",
    "    templateWriter.writerow(['id', 'title'])\n",
    "\n",
    "    for event, elem in etree.iterparse(pathWikiXML, events=('start', 'end')):\n",
    "        tname = strip_tag_name(elem.tag)\n",
    "        if event == 'start':\n",
    "            if tname == 'page':\n",
    "                title = ''\n",
    "                id = -1\n",
    "                redirect = ''\n",
    "                inrevision = False\n",
    "                ns = 0\n",
    "            elif tname == 'revision':\n",
    "                # Do not pick up on revision id's\n",
    "                inrevision = True\n",
    "        else:\n",
    "            if tname == 'title':\n",
    "                title = elem.text\n",
    "            elif tname == 'id' and not inrevision:\n",
    "                id = int(elem.text)\n",
    "            elif tname == 'redirect':\n",
    "                redirect = elem.attrib['title']\n",
    "            elif tname == 'ns':\n",
    "                ns = int(elem.text)\n",
    "            elif tname == 'page':\n",
    "                totalCount += 1\n",
    "                if ns == 10:\n",
    "                    templateCount += 1\n",
    "                    templateWriter.writerow([id, title])\n",
    "                elif len(redirect) > 0:\n",
    "                    articleCount += 1\n",
    "                    articlesWriter.writerow([id, title, redirect])\n",
    "                else:\n",
    "                    redirectCount += 1\n",
    "                    redirectWriter.writerow([id, title, redirect])\n",
    "\n",
    "        if totalCount > 1 and (totalCount % 100000) == 0:\n",
    "            print(\"{:,}\".format(totalCount))\n",
    "\n",
    "    elem.clear()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Total pages: {:,}\".format(totalCount))\n",
    "    print(\"Template pages: {:,}\".format(templateCount))\n",
    "    print(\"Article pages: {:,}\".format(articleCount))\n",
    "    print(\"Redirect pages: {:,}\".format(redirectCount))\n",
    "    print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T02:02:41.733829Z",
     "start_time": "2017-09-13T02:02:41.714771Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-10T03:44:39.002971Z",
     "start_time": "2017-09-10T03:44:38.938307Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achievement': 0.0012221203788573174,\n",
       " 'affection': 0.0006110601894286587,\n",
       " 'aggression': 0.00030553009471432935,\n",
       " 'air_travel': 0.0,\n",
       " 'alcohol': 0.0,\n",
       " 'ancient': 0.00030553009471432935,\n",
       " 'anger': 0.0006110601894286587,\n",
       " 'animal': 0.003360831041857623,\n",
       " 'anonymity': 0.0006110601894286587,\n",
       " 'anticipation': 0.0,\n",
       " 'appearance': 0.002749770852428964,\n",
       " 'art': 0.002444240757714635,\n",
       " 'attractive': 0.0,\n",
       " 'banking': 0.0,\n",
       " 'beach': 0.0,\n",
       " 'beauty': 0.00030553009471432935,\n",
       " 'blue_collar_job': 0.0,\n",
       " 'body': 0.0,\n",
       " 'breaking': 0.00030553009471432935,\n",
       " 'business': 0.010388023220287198,\n",
       " 'car': 0.0006110601894286587,\n",
       " 'celebration': 0.0015276504735716467,\n",
       " 'cheerfulness': 0.0,\n",
       " 'childish': 0.00030553009471432935,\n",
       " 'children': 0.002444240757714635,\n",
       " 'cleaning': 0.00030553009471432935,\n",
       " 'clothing': 0.00030553009471432935,\n",
       " 'cold': 0.00030553009471432935,\n",
       " 'college': 0.0058050717995722576,\n",
       " 'communication': 0.011610143599144515,\n",
       " 'competing': 0.0006110601894286587,\n",
       " 'computer': 0.00977696303085854,\n",
       " 'confusion': 0.0009165902841429881,\n",
       " 'contentment': 0.0,\n",
       " 'cooking': 0.0,\n",
       " 'crime': 0.0,\n",
       " 'dance': 0.00030553009471432935,\n",
       " 'death': 0.00030553009471432935,\n",
       " 'deception': 0.00030553009471432935,\n",
       " 'disappointment': 0.0009165902841429881,\n",
       " 'disgust': 0.0,\n",
       " 'dispute': 0.0009165902841429881,\n",
       " 'divine': 0.00030553009471432935,\n",
       " 'domestic_work': 0.00030553009471432935,\n",
       " 'dominant_heirarchical': 0.00030553009471432935,\n",
       " 'dominant_personality': 0.0,\n",
       " 'driving': 0.0006110601894286587,\n",
       " 'eating': 0.0,\n",
       " 'economics': 0.0030553009471432934,\n",
       " 'emotional': 0.0006110601894286587,\n",
       " 'envy': 0.0006110601894286587,\n",
       " 'exasperation': 0.0,\n",
       " 'exercise': 0.00030553009471432935,\n",
       " 'exotic': 0.00030553009471432935,\n",
       " 'fabric': 0.0009165902841429881,\n",
       " 'family': 0.0012221203788573174,\n",
       " 'farming': 0.0,\n",
       " 'fashion': 0.0030553009471432934,\n",
       " 'fear': 0.0,\n",
       " 'feminine': 0.0012221203788573174,\n",
       " 'fight': 0.00030553009471432935,\n",
       " 'fire': 0.00030553009471432935,\n",
       " 'friends': 0.00030553009471432935,\n",
       " 'fun': 0.0,\n",
       " 'furniture': 0.00030553009471432935,\n",
       " 'gain': 0.0018331805682859762,\n",
       " 'giving': 0.0030553009471432934,\n",
       " 'government': 0.0030553009471432934,\n",
       " 'hate': 0.0006110601894286587,\n",
       " 'healing': 0.0036663611365719525,\n",
       " 'health': 0.0006110601894286587,\n",
       " 'hearing': 0.0012221203788573174,\n",
       " 'help': 0.0015276504735716467,\n",
       " 'heroic': 0.0012221203788573174,\n",
       " 'hiking': 0.002444240757714635,\n",
       " 'hipster': 0.0009165902841429881,\n",
       " 'home': 0.00030553009471432935,\n",
       " 'horror': 0.0,\n",
       " 'hygiene': 0.0,\n",
       " 'independence': 0.0009165902841429881,\n",
       " 'injury': 0.0006110601894286587,\n",
       " 'internet': 0.016804155209288116,\n",
       " 'irritability': 0.0,\n",
       " 'journalism': 0.003360831041857623,\n",
       " 'joy': 0.0,\n",
       " 'kill': 0.0,\n",
       " 'law': 0.0006110601894286587,\n",
       " 'leader': 0.002444240757714635,\n",
       " 'legend': 0.0009165902841429881,\n",
       " 'leisure': 0.0009165902841429881,\n",
       " 'liquid': 0.0,\n",
       " 'listen': 0.0006110601894286587,\n",
       " 'love': 0.0,\n",
       " 'lust': 0.0,\n",
       " 'magic': 0.0,\n",
       " 'masculine': 0.00030553009471432935,\n",
       " 'medical_emergency': 0.0006110601894286587,\n",
       " 'medieval': 0.00030553009471432935,\n",
       " 'meeting': 0.0058050717995722576,\n",
       " 'messaging': 0.010388023220287198,\n",
       " 'military': 0.0006110601894286587,\n",
       " 'money': 0.00030553009471432935,\n",
       " 'monster': 0.0,\n",
       " 'morning': 0.0012221203788573174,\n",
       " 'movement': 0.0006110601894286587,\n",
       " 'music': 0.0009165902841429881,\n",
       " 'musical': 0.0018331805682859762,\n",
       " 'negative_emotion': 0.0009165902841429881,\n",
       " 'neglect': 0.0,\n",
       " 'negotiate': 0.0,\n",
       " 'nervousness': 0.0,\n",
       " 'night': 0.0,\n",
       " 'noise': 0.00030553009471432935,\n",
       " 'occupation': 0.0006110601894286587,\n",
       " 'ocean': 0.00030553009471432935,\n",
       " 'office': 0.010999083409715857,\n",
       " 'optimism': 0.003360831041857623,\n",
       " 'order': 0.002444240757714635,\n",
       " 'pain': 0.0,\n",
       " 'party': 0.0012221203788573174,\n",
       " 'payment': 0.00030553009471432935,\n",
       " 'pet': 0.0,\n",
       " 'philosophy': 0.0030553009471432934,\n",
       " 'phone': 0.00458295142071494,\n",
       " 'plant': 0.002749770852428964,\n",
       " 'play': 0.0009165902841429881,\n",
       " 'politeness': 0.005499541704857928,\n",
       " 'politics': 0.0006110601894286587,\n",
       " 'poor': 0.0,\n",
       " 'positive_emotion': 0.006721662083715246,\n",
       " 'power': 0.0012221203788573174,\n",
       " 'pride': 0.0009165902841429881,\n",
       " 'prison': 0.0,\n",
       " 'programming': 0.019859456156431407,\n",
       " 'rage': 0.0,\n",
       " 'reading': 0.005194011610143599,\n",
       " 'real_estate': 0.0,\n",
       " 'religion': 0.0012221203788573174,\n",
       " 'restaurant': 0.0018331805682859762,\n",
       " 'ridicule': 0.0,\n",
       " 'royalty': 0.0006110601894286587,\n",
       " 'rural': 0.0006110601894286587,\n",
       " 'sadness': 0.0,\n",
       " 'sailing': 0.0,\n",
       " 'school': 0.012832263978001834,\n",
       " 'science': 0.021081576535288724,\n",
       " 'sexual': 0.00030553009471432935,\n",
       " 'shame': 0.00030553009471432935,\n",
       " 'shape_and_size': 0.00458295142071494,\n",
       " 'ship': 0.0,\n",
       " 'shopping': 0.0009165902841429881,\n",
       " 'sleep': 0.0006110601894286587,\n",
       " 'smell': 0.0,\n",
       " 'social_media': 0.007332722273143905,\n",
       " 'sound': 0.00030553009471432935,\n",
       " 'speaking': 0.021387106630003056,\n",
       " 'sports': 0.002749770852428964,\n",
       " 'stealing': 0.0006110601894286587,\n",
       " 'strength': 0.0036663611365719525,\n",
       " 'suffering': 0.00030553009471432935,\n",
       " 'superhero': 0.0,\n",
       " 'surprise': 0.0009165902841429881,\n",
       " 'swearing_terms': 0.0,\n",
       " 'swimming': 0.0,\n",
       " 'sympathy': 0.00488848151542927,\n",
       " 'technology': 0.016498625114573784,\n",
       " 'terrorism': 0.0,\n",
       " 'timidity': 0.00030553009471432935,\n",
       " 'tool': 0.0058050717995722576,\n",
       " 'torment': 0.0,\n",
       " 'tourism': 0.0,\n",
       " 'toy': 0.0009165902841429881,\n",
       " 'traveling': 0.0006110601894286587,\n",
       " 'trust': 0.007638252367858234,\n",
       " 'ugliness': 0.0,\n",
       " 'urban': 0.00030553009471432935,\n",
       " 'vacation': 0.00030553009471432935,\n",
       " 'valuable': 0.0006110601894286587,\n",
       " 'vehicle': 0.0006110601894286587,\n",
       " 'violence': 0.0,\n",
       " 'war': 0.00030553009471432935,\n",
       " 'warmth': 0.0,\n",
       " 'water': 0.0,\n",
       " 'weakness': 0.00030553009471432935,\n",
       " 'wealthy': 0.0009165902841429881,\n",
       " 'weapon': 0.0012221203788573174,\n",
       " 'weather': 0.00030553009471432935,\n",
       " 'wedding': 0.0,\n",
       " 'white_collar_job': 0.0009165902841429881,\n",
       " 'work': 0.011610143599144515,\n",
       " 'worship': 0.0006110601894286587,\n",
       " 'writing': 0.0018331805682859762,\n",
       " 'youth': 0.00030553009471432935,\n",
       " 'zest': 0.0}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon.analyze(input_doc, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T02:37:26.487666Z",
     "start_time": "2017-09-13T02:37:26.426132Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"machine_learning\", \"data_science\", \"deep_learning\", \"computer_vision\", \"Machine_learning\", \"natural_language_processing\", \"data_analysis\", \"Machine_Learning\", \"bioinformatics\", \"artificial_intelligence\", \"neural_nets\", \"big_data\", \"neural_networks\", \"Computer_science\", \"AI_research\", \"software_engineering\", \"robotics\", \"programming\", \"computer_science\", \"complexity_theory\", \"computing\", \"genetic_algorithms\", \"quantum_computing\", \"computer_scientists\", \"computational\", \"computer_programming\", \"software_development\", \"computer_architecture\", \"cognitive_science\", \"data_scientist\", \"control_theory\", \"data_analytics\", \"distributed_systems\", \"practical_applications\", \"econometrics\", \"applied_mathematics\", \"scientific_computing\", \"information_technology\", \"algorithms\", \"subfield\", \"theoretical_physics\", \"growing_field\", \"other_fields\", \"mathematics\", \"computer_scientist\", \"synthetic_biology\", \"distributed_computing\", \"cryptography\", \"neuroscience\", \"Artificial_Intelligence\", \"numerical_analysis\", \"advanced_mathematics\", \"materials_science\", \"embedded_systems\", \"programming_languages\", \"NLP\", \"real-world_applications\", \"informatics\", \"related_fields\", \"computer_technology\", \"other_disciplines\", \"engineering\", \"cognitive_psychology\", \"higher_mathematics\", \"mathematical_physics\", \"AI_development\", \"complex_systems\", \"math_background\", \"business_intelligence\", \"computational_linguistics\", \"many_different_fields\", \"pure_mathematics\", \"software_design\", \"dynamical_systems\", \"various_fields\", \"web_development\", \"discrete_mathematics\", \"NLP\", \"machine_intelligence\", \"theoretical_side\", \"general_AI\", \"broad_field\", \"numerical_methods\", \"data_scientists\", \"Software_engineering\"]\n"
     ]
    }
   ],
   "source": [
    "tokens = '''artificial_intelligence machine_learning data_science'''\n",
    "tokens = tokens.split(\" \")\n",
    "lexicon.create_category(\"category_name\",tokens, model=\"reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T03:39:07.659382Z",
     "start_time": "2017-09-13T03:39:07.299471Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamburgers,french fries,soda\n",
      "['french fries', 'French fries', 'potato chips', 'fries', 'hamburgers', 'hotdogs', 'hot dogs', 'ketchup', 'nacho cheese', 'ranch dressing', 'chicken nuggets', 'sandwiches', 'nachos', 'bagels', 'pop tarts', 'oreos', 'white bread', 'chicken wings', 'condiments', 'crackers', 'tater tots', 'hummus', 'onion rings', 'tortilla chips', 'soda', 'tomato soup', 'corn dogs', 'sandwich', 'bread sticks', 'pickles', 'bacon', 'milkshakes', 'ice cream', 'pretzels', 'beef jerky', 'cereal', 'garlic bread', 'ramen', 'salad dressing', 'hamburger', 'frozen pizza', 'chicken strips', 'Oreos', 'guacamole', 'pancakes', 'icecream', 'salad', 'soup', 'buffalo wings', 'cheeseburger', 'chicken fingers', 'nutella', 'sandwhich', 'hot wings', 'saltines', 'instant noodles', 'cheeseburgers', 'cheese fries', 'soda', 'lunch meat', 'peanut butter', 'deli meats', 'bread', 'PB&amp;J', 'apple sauce', 'chicken sandwich', 'pepperoni pizza', 'crisps', 'hot sauce', 'apple slices', 'Fritos', 'pizza', 'mashed potatoes', 'sausages', 'burger', 'Ritz crackers', 'French toast', 'hot chips', 'hash browns', 'mayo', 'Nutella', 'cheese dip', 'yogurt', 'dipping sauce', 'BBQ sauce', 'hamburger buns', 'cheese sticks', 'breadsticks', 'burritos', 'Doritos', 'fried food', 'applesauce', 'sweet potato fries', 'cheese burgers', 'celery sticks', 'grilled cheese sandwiches', 'cheese burger', 'tartar sauce', 'gravy', 'hamburgers', 'pretzels', 'potato chips', 'french fries', 'crackers', 'burgers', 'ice cream', 'sodas', 'soda', 'cheeseburgers', 'doughnuts', 'bagels', 'French fries', 'lemonade', 'iced tea', 'hamburger', 'fries', 'fried chicken', 'hot dogs', 'orange juice', 'peanut butter', 'mineral water', 'peanuts', 'ketchup', 'candy bars', 'muffins', 'Oreos', 'tacos', 'biscuits', 'brownies', 'popcorn', 'granola', 'chicken wings', 'pickles', 'apple juice', 'snack', 'cereal', 'burger', 'cold cuts', 'knishes', 'espresso', 'margaritas', 'hot dog', 'candy bar', 'white bread', 'bologna', 'martinis', 'cornflakes', 'burritos', 'frozen yogurt', 'tequila', 'milkshakes', 'oatmeal', 'cheeseburger', 'sandwiches', 'nachos', 'pizza', 'tuna fish', 'latte', 'sausages', 'lox', 'cappuccino', 'milkshake', 'Big Mac', 'croissants', 'buns', 'hot chocolate', 'fruit juice', 'snacks', 'fresh fruit', 'sparkling water', 'soft drink', 'franks', 'Cokes', 'bagel', 'scotch', 'bratwurst', 'pies', 'tortillas', 'seltzer', 'gin', 'cupcakes', 'spaghetti', 'fresh bread', 'potato salad', 'cat food', 'tortilla chips', 'green tea', 'baked potatoes', 'tonics', 'root beer', 'bourbon', 'salad bar', 'Scotch', 'scrambled eggs', 'burrito', 'waffles', 'pastrami', 'roast beef', 'french fries', 'nachos', 'soda', 'French fries', 'pretzels', 'chicken nuggets', 'chocolate milk', 'Pepsi', 'Dr. Pepper', 'doughnuts', 'root beer', 'pepsi', 'sodas', 'cookie dough', 'chocolate ice cream', 'vanilla ice cream', 'apple juice', 'Mountain Dew', 'chicken wings', 'milkshake', 'hamburgers', 'cheeseburger', 'chocolate milkshake', 'hot dogs', 'donuts', 'lemonade', 'yogurt', 'ham sandwich', 'brownie', 'brownies', 'fruit salad', 'diet coke', 'pepperoni pizza', 'chicken sandwich', 'bagels', 'hotdogs', 'tuna', 'doughnut', 'fries', 'crackers', 'pickles', 'Cheetos', 'orange chicken', 'smoothies', 'Sprite', 'big bowl', 'chips', 'chocolate chip', 'burgers', 'smoothie', 'hot sauce', 'coke', 'sundae', 'peanuts', 'hot dog', 'gummy bears', 'fruit punch', 'sweet tea', 'sprite', 'sushi', 'potato chips', 'mountain dew', 'grapes', 'chicken salad', 'Doritos', 'Coke', 'chili', 'oreos', 'ice tea', 'cheese pizza', 'biscuits', 'huge bowl', 'ravioli', 'chocolate chip cookies', 'cheese burger', 'donut', 'Oreos', 'fried rice', 'milkshakes', 'cheetos', 'toppings', 'cheesecake', 'gummy worms', 'strawberry milkshake', 'Gatorade', 'shrimp', 'chocolate cake', 'crisps', 'salads', 'sandwhiches', 'mash potatoes', 'whole box', 'iced tea', 'juice', 'lollies', 'sour patch kids', 'cheeseburgers', 'slushy', 'fruit snacks', 'Doritos']\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def capture_from_stdout(function):\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        function()\n",
    "    return f.getvalue()\n",
    "\n",
    "def category_from_keywords(com_sep_keyword_string, model='all'):\n",
    "    '''Call Empath's create category. Model options: \"fiction\",\"nytimes\",\"reddit\",\"all\"   '''\n",
    "    keywords = com_sep_keyword_string.split(\",\") #split into individual items with comma separator\n",
    "    \n",
    "    #replace spaces with underscores for Empath's lexicon format\n",
    "    keywords = [keyword.replace(\" \", \"_\") for keyword in keywords]\n",
    "\n",
    "    category_name = keywords[0] + \" \" + keywords[1] #name the category after the first two keywords\n",
    "    if model  == 'all':\n",
    "        category_terms = []\n",
    "        for model in ['reddit','nytimes','fiction']:\n",
    "            f = io.StringIO()\n",
    "            with redirect_stdout(f):\n",
    "                lexicon.create_category(category_name, keywords, model=model, write=False)\n",
    "            model_terms = f.getvalue().strip().replace(\"[]\",\"\")\n",
    "            if model_terms:\n",
    "                model_terms = model_terms[1:-1] #exclude enclosing brackets\n",
    "                model_terms = model_terms.replace('\"','').split(\", \")\n",
    "                category_terms.append(model_terms)\n",
    "    else:\n",
    "        f = io.StringIO()\n",
    "        with redirect_stdout(f):\n",
    "            lexicon.create_category(category_name, keywords, model=model)\n",
    "        category_terms = f.getvalue().strip()\n",
    "    \n",
    "    category_terms = [term for model_terms in category_terms for term in model_terms] #flatten lists\n",
    "    category_terms = [term.replace('_',' ') for term in category_terms] #re-separate on underscores\n",
    "    return category_terms\n",
    "\n",
    "keywords = \"hamburgers,french fries,soda\"\n",
    "expanded_keywords = category_from_keywords(keywords)\n",
    "print(keywords)\n",
    "print(expanded_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T04:22:35.343457Z",
     "start_time": "2017-09-13T04:22:32.922733Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hamburgers', 'french fries', 'soda']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davideverling/anaconda/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: no parser specified warning\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "def get_wiki_text_for_keywords(keywords):\n",
    "    all_text = \"\"\n",
    "    if type(keywords)==str:\n",
    "        keywords = keywords.split(\",\")\n",
    "        \n",
    "    print(keywords)    \n",
    "    for keyword in keywords:\n",
    "        wiki_data = get_wiki_page(keyword, summary=True, content=True)\n",
    "        if wiki_data != None:\n",
    "            try:\n",
    "                #summary = wiki_data['summary']\n",
    "                content = wiki_data['content']\n",
    "                all_text += content\n",
    "            except KeyError:\n",
    "                continue\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "wiki_blob = get_wiki_text_for_keywords(keywords)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T17:59:10.873708Z",
     "start_time": "2017-09-12T17:59:10.866202Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T04:23:41.158859Z",
     "start_time": "2017-09-13T04:23:41.123573Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heat will not be as an alternative.\n",
      "The hamburger, usually fried, is served between two slices of bread on the dinner menu.\n",
      "Lotteria is a local version of fish and chips.\n",
      "This is where the burger happened largely at the St. Louis World's Fair.\n",
      "Poutine has a growing number of different cheeses, toppings, and sauces.\n"
     ]
    }
   ],
   "source": [
    "# generate a corpus for markovify by reading wikipedia articles for lexical category words\n",
    "import markovify\n",
    "text = wiki_blob\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T18:01:23.594143Z",
     "start_time": "2017-09-12T18:01:23.591123Z"
    }
   },
   "source": [
    "## Synonym Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Wordnet Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-10T04:33:45.006093Z",
     "start_time": "2017-09-10T04:33:44.995436Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning 0 NLTK ID: java.n.01\n",
      "Definition: an island in Indonesia to the south of Borneo; one of the world's most densely populated regions\n",
      "Synonyms: Java\n",
      "Meaning 1 NLTK ID: coffee.n.01\n",
      "Definition: a beverage consisting of an infusion of ground coffee beans\n",
      "Synonyms: coffee, java\n",
      "Meaning 2 NLTK ID: java.n.03\n",
      "Definition: a platform-independent object-oriented programming language\n",
      "Synonyms: Java\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "for i,j in enumerate(wn.synsets('java')):\n",
    "    print(\"Meaning\",i, \"NLTK ID:\", j.name())\n",
    "    print(\"Definition:\",j.definition())\n",
    "    print(\"Synonyms:\", \", \".join(j.lemma_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T18:12:23.541716Z",
     "start_time": "2017-09-12T18:12:23.522920Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('small.n.01') []\n",
      "Synset('small.n.02') []\n",
      "Synset('small.a.01') ['a little dining room', 'a little house', 'a small car', 'a little (or small) group']\n",
      "    Synset('atomic.s.03')\n",
      "    Synset('bantam.s.01')\n",
      "    Synset('bitty.s.01')\n",
      "    Synset('dinky.s.01')\n",
      "    Synset('dwarfish.s.01')\n",
      "    Synset('elfin.s.02')\n",
      "    Synset('gnomish.s.01')\n",
      "    Synset('half-size.s.01')\n",
      "    Synset('infinitesimal.s.01')\n",
      "    Synset('lesser.s.02')\n",
      "    Synset('micro.s.01')\n",
      "    Synset('microscopic.s.04')\n",
      "    Synset('miniature.s.01')\n",
      "    Synset('minuscule.s.03')\n",
      "    Synset('olive-sized.s.01')\n",
      "    Synset('pocket-size.s.02')\n",
      "    Synset('puny.s.02')\n",
      "    Synset('slender.s.04')\n",
      "    Synset('small-scale.s.01')\n",
      "    Synset('smaller.s.01')\n",
      "    Synset('smallish.s.01')\n",
      "    Synset('subatomic.s.02')\n",
      "    Synset('undersize.s.01')\n",
      "Synset('minor.s.10') ['a small business', 'a newspaper with a modest circulation', 'small-scale plans', 'a pocket-size country']\n",
      "    Synset('limited.a.01')\n",
      "Synset('little.s.03') ['what a big little boy you are', 'small children']\n",
      "    Synset('young.a.01')\n",
      "Synset('small.s.04') ['a series of death struggles with small time in between']\n",
      "    Synset('little.a.02')\n",
      "Synset('humble.s.01') ['a humble cottage', 'a lowly parish priest', 'a modest man of the people', 'small beginnings']\n",
      "    Synset('inferior.a.01')\n",
      "Synset('little.s.07') ['little a', 'small a', \"e.e.cummings's poetry is written all in minuscule letters\"]\n",
      "    Synset('lowercase.a.01')\n",
      "Synset('little.s.05') ['a little voice', 'a still small voice']\n",
      "    Synset('soft.a.03')\n",
      "Synset('small.s.08') ['a small misty rain']\n",
      "    Synset('fine.a.05')\n",
      "Synset('modest.s.02') ['a modest salary', 'modest inflation', 'helped in my own small way']\n",
      "    Synset('moderate.a.01')\n",
      "Synset('belittled.s.01') ['her comments made me feel small']\n",
      "    Synset('decreased.a.01')\n",
      "Synset('small.r.01') ['think small']\n"
     ]
    }
   ],
   "source": [
    "for ss in wn.synsets('small'):\n",
    "    print(ss, ss.examples())\n",
    "    for sim in ss.similar_tos():\n",
    "        print('    {}'.format(sim))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T18:19:24.610008Z",
     "start_time": "2017-09-12T18:19:24.546001Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning 0 NLTK ID: language.n.01\n",
      "Hypernyms: communication\n",
      "Hyponyms: artificial_language, barrage, bombardment, outpouring, onslaught, dead_language, indigenous_language, lingua_franca, interlanguage, koine, metalanguage, native_language, natural_language, tongue, object_language, target_language, sign_language, signing, slanguage, source_language, string_of_words, word_string, linguistic_string, superstrate, superstratum, usage, words\n",
      "Meaning 1 NLTK ID: speech.n.02\n",
      "Hypernyms: auditory_communication\n",
      "Hyponyms: conversation, dictation, discussion, give-and-take, word, idiolect, monologue, non-standard_speech, pronunciation, orthoepy, saying, expression, locution, soliloquy, monologue, spell, magic_spell, magical_spell, charm, words\n",
      "Meaning 2 NLTK ID: lyric.n.01\n",
      "Hypernyms: text, textual_matter\n",
      "Hyponyms: love_lyric\n",
      "Meaning 3 NLTK ID: linguistic_process.n.02\n",
      "Hypernyms: higher_cognitive_process\n",
      "Hyponyms: reading\n",
      "Meaning 4 NLTK ID: language.n.05\n",
      "Hypernyms: faculty, mental_faculty, module\n",
      "Hyponyms: \n",
      "Meaning 5 NLTK ID: terminology.n.01\n",
      "Hypernyms: word\n",
      "Hyponyms: markup_language, toponymy, toponomy\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "for i,j in enumerate(wn.synsets('language')):\n",
    "    print(\"Meaning\",i, \"NLTK ID:\", j.name())\n",
    "    hypernyms = list(chain(*[l.lemma_names() for l in j.hypernyms()]))\n",
    "    hyponyms = list(chain(*[l.lemma_names() for l in j.hyponyms()]))\n",
    "    print(\"Hypernyms:\", \", \".join(hypernyms))\n",
    "    print(\"Hyponyms:\", \", \".join(hyponyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### PyDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T19:55:03.810398Z",
     "start_time": "2017-09-12T19:55:02.890918Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davideverling/anaconda/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: no parser specified warning\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Adjective': ['of or resembling alabaster'],\n",
      " 'Noun': ['a compact fine-textured, usually white gypsum used for carving',\n",
      "          'a hard compact kind of calcite',\n",
      "          'a very light white']}\n"
     ]
    }
   ],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "def dict_from_doc_tokens(unigram_tokens):\n",
    "    '''Calls PyDictionary (which calls thesaurus.com) to retrieve synonyms'''\n",
    "    pydict=PyDictionary(unigram_tokens)\n",
    "    \n",
    "    dictionary = defaultdict(dict)\n",
    "    for word in unigram_tokens:\n",
    "        meaning = pydict.meaning(word)\n",
    "        synonyms = pydict.synonym(word)\n",
    "        dictionary[word] = {\"meaning\":meaning, \"synonyms\":synonyms}\n",
    "    return dictionary\n",
    "\n",
    "def get_meanings(word, pos=\"all\"):\n",
    "    '''Returns meaning definitions from an existing pydictionary, or None if no meanings found'''\n",
    "    meanings = dictionary[word]['meaning']\n",
    "\n",
    "    if pos.startswith(\"all\"):\n",
    "        return meanings\n",
    "    if pos.startswith(\"N\"):\n",
    "        return meanings['Noun']\n",
    "    if pos.startswith(\"V\"):\n",
    "        return meanings[\"Verb\"]\n",
    "    if pos.startswith(\"J\"):\n",
    "        return meanings[\"Adjective\"]\n",
    "    if pos.startswith(\"RB\"):\n",
    "        return meanings[\"Adverb\"]\n",
    "    \n",
    "    return meanings\n",
    "\n",
    "tokens = ['alabaster']\n",
    "dictionary = dict_from_doc_tokens(tokens)\n",
    "pprint(get_meanings(tokens[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Proselint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T04:49:20.120554Z",
     "start_time": "2017-09-12T04:49:19.967746Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('typography.symbols.ellipsis',\n",
      "  \"'...' is an approximation, use the ellipsis symbol '…'.\",\n",
      "  8,\n",
      "  1,\n",
      "  1255,\n",
      "  1257,\n",
      "  2,\n",
      "  'warning',\n",
      "  None),\n",
      " ('typography.symbols.curly_quotes',\n",
      "  'Use curly quotes “”, not straight quotes \"\". Found 4 times elsewhere.',\n",
      "  70,\n",
      "  899,\n",
      "  14585,\n",
      "  14587,\n",
      "  2,\n",
      "  'warning',\n",
      "  '“ or ”'),\n",
      " ('typography.symbols.curly_quotes',\n",
      "  'Use curly quotes “”, not straight quotes \"\".',\n",
      "  87,\n",
      "  621,\n",
      "  19061,\n",
      "  19063,\n",
      "  2,\n",
      "  'warning',\n",
      "  '“ or ”')]\n"
     ]
    }
   ],
   "source": [
    "import proselint\n",
    "from pprint import pprint\n",
    "\n",
    "suggestions = proselint.tools.lint(input_doc)\n",
    "pprint(suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T02:44:08.406450Z",
     "start_time": "2017-09-12T02:44:08.345351Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## TextGenRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textgenrnn import textgenrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T03:23:36.743972Z",
     "start_time": "2017-09-12T03:23:35.894539Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 779 ms, sys: 45.3 ms, total: 825 ms\n",
      "Wall time: 845 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "textgen = textgenrnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T03:24:26.249467Z",
     "start_time": "2017-09-12T03:24:09.981258Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The street to the country who was a company with the successful to the state and the president and the star the support to the street and so many company and the first time to be a star to the stran\n",
      "\n",
      "I want to be a lot of the most of the first time to start the most the star for the stream in the street of the sunders and be a beautiful things and the first time to start the states with a presid\n",
      "\n",
      "I was been a start of the company is a stream and a lot in the first time to be a comparing the statement of the control of the state state of the store in the most favorite and a guy and the statem\n",
      "\n",
      "The president and the compare the story of the Season 2 on THE REALLY OUT on Democratic Contrest Card in the president and the star to the stream to the star to the street to show the best of the be\n",
      "\n",
      "A comparison of the country of the story of the story of the second and he was the stream in the street to make the world was a starts and being a months at the state and show and a stranger in the \n",
      "\n",
      "CPU times: user 15.4 s, sys: 12.7 s, total: 28.2 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "textgen.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T03:26:44.246372Z",
     "start_time": "2017-09-12T03:26:34.669384Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine learning decline of a moment when you did the most little in the '\n",
      " 'grick down.',\n",
      " 'Machine learning the an about the police in the one of the most meeting for '\n",
      " 'the look of the deal to make the first company of changed by the behind 10 '\n",
      " 'thank you say they should be about to look at t',\n",
      " 'Machine learning a with the opener and having the country means the standow '\n",
      " 'in a family of the morning Space Are Official Weekend',\n",
      " 'Machine learning a shows at the wedding of its new first time to ever are '\n",
      " 'the states to the pressure course and believe the moment and display because '\n",
      " 'of the first time the street how to be comments',\n",
      " 'Machine learning a tonight than an a world and share to clea...']\n"
     ]
    }
   ],
   "source": [
    "generated_texts = textgen.generate(n=5, prefix=\"Machine learning\", temperature=0.5, return_as_list=True)\n",
    "pprint(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T04:08:30.069530Z",
     "start_time": "2017-09-12T03:27:38.350845Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "texts = [input_doc]\n",
    "\n",
    "textgen.train_on_texts(texts, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T17:58:23.744916Z",
     "start_time": "2017-09-12T17:57:32.960247Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Normal Output\n",
      "---\n",
      "Computational linguistics is often to develop language the probabilities for the understanding of the probability that it is also used to create a system's linguistic problem and computer program of\n",
      "\n",
      "Computational linguistics is often to also been developed by computers to understanding human language was called a project conversational approach.\n",
      "\n",
      "The ability to the structure of these scientist \n",
      "\n",
      "Computational linguistics is also been developed in the probabilities which also many has been acoustically been consisting an effort to scope of a computational and so the understanding of the user\n",
      "\n",
      "Computational linguistics is often to develop language which computational linguistics is only also used to create as the children and computational linguistics as some of the problem of language wh\n",
      "\n",
      "Computational linguistics is often to create a system's linguistic development of a computational and computer probabilities that can make a computational approach to this environment of language to\n",
      "\n",
      "---\n",
      "High Temperature Output\n",
      "---\n",
      "Computational linguistics allows, and learn to mould other has been meticulouslines, and comprehension and sever-based because around only other annowing users were admonies if children and computer\n",
      "\n",
      "Computational linguistics develop event user's of a search engine the information in the prescents of has naturally forms an verson ELIZA user's time models.\n",
      "\n",
      "In 691s and perform to specifical compu\n",
      "\n",
      "Computational linguistics devoten light not retrieved been questioned for the properating of probabilities when artificial intelligence intelligence, inclodubies, such as \"I communing\" of new bocks \n",
      "\n",
      "Computational linguistics tell trisify identify human corpucars. Original linguistics, the structures and created. There are capable of the field of used to created inaccessible triggen sheestilites\n",
      "\n",
      "Compulurity patterns mucture computational languarts computational linguistics is also been evolved in results structures, and well a large dictically with computational linguistics developmental pe\n",
      "\n",
      "---\n",
      "Prefix Output\n",
      "---\n",
      "Now matched the information in the primitian of language was can be better into any one of them. Human language from a language parsing programs who developed answers in the probability to include t\n",
      "\n",
      "Now words the information in the problem which a system also provide the learning proposed the probability that impressive, included the linguistic structure of the approaches with the user in the p\n",
      "\n",
      "Now are understanding of the life of thought.[23] Today that well as the computational linguistics is also understanding the structural language was can be better interaction about the language work\n",
      "\n",
      "Now work in which also used to apply a computational linguistics is also understand the computer to model which are conversational approaches and the interpreting of the problem and computer program\n",
      "\n",
      "Now work in which also used to translate structures which also mapping, and comprehension and structure of language was can be retrieved before the linguistic study of linguistic development of arti\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"---\\nNormal Output\\n---\")\n",
    "textgen.generate(5)\n",
    "print(\"---\\nHigh Temperature Output\\n---\")\n",
    "textgen.generate(5, temperature=1.0)\n",
    "print(\"---\\nPrefix Output\\n---\")\n",
    "textgen.generate(5, prefix=\"N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Extractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T23:42:00.931822Z",
     "start_time": "2017-09-12T23:42:00.928831Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T00:48:48.353148Z",
     "start_time": "2017-09-13T00:48:48.139151Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Computational linguistics originated with efforts in the United States in the 1950s to use computers to automatically translate texts from foreign languages, particularly Russian scientific journals, into English.[3] Since computers can make arithmetic calculations much faster and more accurately than humans, it was thought to be only a short matter of time before they could also begin to process language.[4] Computational and quantitative methods are also used historically in attempted reconstruction of earlier forms of modern languages and subgrouping modern languages into language families.\\nUsing models, it has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span.[16] This was simultaneously posed as a reason for the long developmental period of human children.[16] Both conclusions were drawn because of the strength of the neural network which the project created.\\nCrucially, these robots were able to acquire functioning word-to-meaning mappings without needing grammatical structure, vastly simplifying the learning process and shedding light on information which furthers the current understanding of linguistic development.\\nOne of the original theoretical theses on internalization of grammar and structure of language proposed two types of models.[14] In these models, rules or patterns learned increase in strength with the frequency of their encounter.[14] The work also created a question for computational linguists to answer: how does an infant learn a specific and non-normal grammar (Chomsky Normal Form) without learning an overgeneralized version and getting stuck?[14] Theoretical efforts like these set the direction for research to go early in the lifetime of a field of study, and are crucial to the growth of the field.\\nStructural information about languages allows for the discovery and implementation of similarity recognition between pairs of text utterances.[21] For instance, it has recently been proven that based on the structural information present in patterns of human discourse, conceptual recurrence plots can be used to model and visualize trends in data and create reliable measures of similarity between natural textual utterances.[21] This technique is a strong tool for further probing the structure of human discourse.\\nUsing linguistic input from humans, algorithms have been constructed which are able to modify a system's style of production based on a factor such as linguistic input from a human, or more abstract factors like politeness or any of the five main dimensions of personality.[26] This work takes a computational approach via parameter estimation models to categorize the vast array of linguistic styles we see across individuals and simplify it for a computer to work in the same way, making human-computer interaction much more natural.\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_summary(text, ratio=0.25):\n",
    "    '''Wraps gensim summarize()'''\n",
    "    return summarize(text, ratio)\n",
    "extract_summary(input_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-12T20:16:55.145738Z",
     "start_time": "2017-09-12T20:16:54.508504Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computers',\n",
       " 'computational',\n",
       " 'linguistics',\n",
       " 'linguistic',\n",
       " 'linguists',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'models',\n",
       " 'model',\n",
       " 'modeled',\n",
       " 'languages',\n",
       " 'particularly',\n",
       " 'word',\n",
       " 'words',\n",
       " 'translations',\n",
       " 'translated',\n",
       " 'approaches',\n",
       " 'human',\n",
       " 'humans',\n",
       " 'structural',\n",
       " 'structure',\n",
       " 'structures',\n",
       " 'speech',\n",
       " 'programs',\n",
       " 'program',\n",
       " 'programming',\n",
       " 'based',\n",
       " 'base',\n",
       " 'rule',\n",
       " 'modeling',\n",
       " 'translate',\n",
       " 'texts',\n",
       " 'processing',\n",
       " 'process',\n",
       " 'include',\n",
       " 'including',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'development',\n",
       " 'developing',\n",
       " 'develops',\n",
       " 'develop',\n",
       " 'developed',\n",
       " 'developer',\n",
       " 'intelligence',\n",
       " 'intelligently',\n",
       " 'fields',\n",
       " 'information',\n",
       " 'theoretical',\n",
       " 'text',\n",
       " 'interactive',\n",
       " 'approach',\n",
       " 'examples',\n",
       " 'example',\n",
       " 'study',\n",
       " 'studies',\n",
       " 'studied',\n",
       " 'testing',\n",
       " 'test',\n",
       " 'converse',\n",
       " 'naturally',\n",
       " 'origins',\n",
       " 'originated',\n",
       " 'originally',\n",
       " 'original',\n",
       " 'learned',\n",
       " 'learn',\n",
       " 'learning',\n",
       " 'attempted',\n",
       " 'attempts',\n",
       " 'attempting',\n",
       " 'attempt',\n",
       " 'search',\n",
       " 'searches',\n",
       " 'work',\n",
       " 'works',\n",
       " 'included',\n",
       " 'applying',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " 'understand',\n",
       " 'understanding',\n",
       " 'understands',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'data',\n",
       " 'methods',\n",
       " 'method',\n",
       " 'developmental',\n",
       " 'possibility',\n",
       " 'possibilities',\n",
       " 'possible',\n",
       " 'questions',\n",
       " 'question',\n",
       " 'paper',\n",
       " 'papers',\n",
       " 'patterns',\n",
       " 'pattern',\n",
       " 'environment',\n",
       " 'create',\n",
       " 'working',\n",
       " 'improved',\n",
       " 'improve',\n",
       " 'normal',\n",
       " 'written',\n",
       " 'empirically',\n",
       " 'tested',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'allowing',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'input',\n",
       " 'presented',\n",
       " 'cognitive',\n",
       " 'created',\n",
       " 'creating',\n",
       " 'tools',\n",
       " 'tool',\n",
       " 'recognition',\n",
       " 'section',\n",
       " 'possibly',\n",
       " 'called',\n",
       " 'conversational',\n",
       " 'comprehension',\n",
       " 'use',\n",
       " 'remained',\n",
       " 'remains',\n",
       " 'topics',\n",
       " 'topic',\n",
       " 'machines',\n",
       " 'present',\n",
       " 'general',\n",
       " 'generated',\n",
       " 'generate',\n",
       " 'generating',\n",
       " 'treebank',\n",
       " 'attention',\n",
       " 'systems',\n",
       " 'better',\n",
       " 'provides',\n",
       " 'certain',\n",
       " 'discovery',\n",
       " 'discoveries',\n",
       " 'modern',\n",
       " 'conversations',\n",
       " 'conversation',\n",
       " 'scientific',\n",
       " 'forms',\n",
       " 'form',\n",
       " 'statistical',\n",
       " 'statistics',\n",
       " 'unknown',\n",
       " 'probability',\n",
       " 'probabilities',\n",
       " 'contents',\n",
       " 'content',\n",
       " 'involvement',\n",
       " 'involve',\n",
       " 'involves',\n",
       " 'involving',\n",
       " 'citations',\n",
       " 'english',\n",
       " 'efforts',\n",
       " 'effort',\n",
       " 'sentence',\n",
       " 'eliza',\n",
       " 'interaction',\n",
       " 'automated',\n",
       " 'vastly',\n",
       " 'turing',\n",
       " 'research',\n",
       " 'researchers',\n",
       " 'especially',\n",
       " 'sections',\n",
       " 'known',\n",
       " 'matching',\n",
       " 'matches',\n",
       " 'matched',\n",
       " 'user',\n",
       " 'users',\n",
       " 'wide',\n",
       " 'widely',\n",
       " 'discourse',\n",
       " 'engines',\n",
       " 'engine',\n",
       " 'scientists',\n",
       " 'scientist',\n",
       " 'providing',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'empirical',\n",
       " 'rules',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'recognized',\n",
       " 'recognize',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'documents',\n",
       " 'document']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords(input_doc).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
