{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_ling_doc_path = '../corpus_data/comp_ling.txt'\n",
    "\n",
    "with open('../corpus_data/comp_ling.txt') as f:\n",
    "    input_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'textacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-4b627ce094e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'textacy'"
     ]
    }
   ],
   "source": [
    "from textacy import text_stats\n",
    "from textacy import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_doc = '''\n",
    "# Oxygen is a chemical element with symbol O and atomic number 8. A member of the chalcogen group on the periodic table, it is a highly reactive nonmetal and oxidizing agent that forms oxides with most elements as well as other compounds.\n",
    "# '''\n",
    "\n",
    "def as_spacy_doc(raw_doc):\n",
    "    return nlp(doc)\n",
    "\n",
    "def as_textacy_doc(doc):\n",
    "    doc = textacy.Doc(nlp(doc))\n",
    "    return doc\n",
    "\n",
    "doc = as_textacy_doc(input_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Today, \n",
       " \n",
       " The Association for Computational Linguistics, \n",
       " \n",
       " ..., 1, 2, 2.1, 2.2, 2.3, 2.3.1, 2.3.2, \n",
       " 2.4, 3, 4, 5, 6, 7\tReferences, 8\tExternal, United States, 1950s, Russian, 1960s, one, one, one, four, one, Crucially, Pólya, modern-day, English, One, One, English, IBM, over 4.5 million words, American, English, One, two, early in the lifetime of a field of study, English, Japanese, Japanese, Japanese, October 2015, only half, Alan Turing, Turing Test, 1950, Alan Turing, one day, two, one, Turing, Today, Turing, Joseph Weizenbaum, MIT, ELIZA, One, ELIZA, Joseph Weizenbaum, MIT, 1966, Rogerian, ELIZA, ELIZA, ELIZA, first, first, Markov, translation.[25] The, German, French, first, five, ELIZA, Siri, Bayesian, Bledsoe, Browing, 1959, one, Bayesian, Mosteller, Wallace, 1963, Federalist Papers, Madison, 1971, Terry Winograd, SHRDLU, SHRDLU, NASA, LUNAR, Apollo, 1960s and 1970s, Markov, Rabiner, 1989.[32, late 70s, IBM, Bayesian, Apple, Siri, Google Translate, Social, Twitter, four, four]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_named_entities(doc):\n",
    "    nes = textacy.extract.named_entities(doc)\n",
    "    return [ne for ne in nes]\n",
    "\n",
    "get_named_entities(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'automated_readability_index': 22.352141689526015,\n",
       " 'coleman_liau_index': 16.682927688311693,\n",
       " 'flesch_kincaid_grade_level': 19.052077766395673,\n",
       " 'flesch_readability_ease': 19.26507900874003,\n",
       " 'gulpease_index': 41.068861371186955,\n",
       " 'gunning_fog_index': 22.472696136278014,\n",
       " 'lix': 72.24939361765064,\n",
       " 'smog_index': 18.80095838887095,\n",
       " 'wiener_sachtextformel': 12.265282847241465}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_readability_stats(textacy_doc):\n",
    "    ts = text_stats.TextStats(textacy_doc)\n",
    "    return ts.readability_stats\n",
    "\n",
    "get_readability_stats(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', 0.02416255077377969),\n",
       " ('linguistic', 0.02043304879128454),\n",
       " ('computational', 0.01746222784332213),\n",
       " ('human', 0.011689631474031183),\n",
       " ('computer', 0.011480717062340147),\n",
       " ('model', 0.011188761077018177),\n",
       " ('word', 0.011081683346972649),\n",
       " ('approach', 0.008327125663422037),\n",
       " ('speech', 0.007841891597153119),\n",
       " ('program', 0.007792902818308204)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.keyterms.key_terms_from_semantic_network(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-124bc5e0cfcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikipedia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'datasets'"
     ]
    }
   ],
   "source": [
    "\n",
    "wp = Wikipedia(lang='en', version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'textacy' has no attribute 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-d9a2aee523e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# textacy.corpus.Corpus.load(corpus_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapitolWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTaggedText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkovify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'textacy' has no attribute 'datasets'"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "corpus_path = \"/corpus_data/comp_ling.txt\"\n",
    "corpus_name = \"comp_ling\"\n",
    "# corpus = textacy.Corpus.load(corpus_path, \n",
    "#         name=corpus_name, compression='gzip')\n",
    "\n",
    "#corpus = textacy.Corpus(\"en\", texts=[as_textacy_doc(input_doc)])\n",
    "# textacy.corpus.Corpus.load(corpus_path)\n",
    "\n",
    "corpus = textacy.datasets.CapitolWords()\n",
    "\n",
    "class TaggedText(markovify.Text):\n",
    "\n",
    "    def sentence_split(self, text):\n",
    "        \"\"\"\n",
    "        Splits full-text string into a list of sentences.\n",
    "        \"\"\"\n",
    "        sentence_list = []\n",
    "        for doc in corpus:\n",
    "            sentence_list += list(doc.sents)\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    def word_split(self, sentence):\n",
    "        \"\"\"\n",
    "        Splits a sentence into a list of words.\n",
    "        \"\"\"\n",
    "        return [\"::\".join((word.orth_,word.pos_)) for word in sentence]\n",
    "\n",
    "    def word_join(self, words):\n",
    "        sentence = \" \".join(word.split(\"::\")[0] for word in words)\n",
    "        return sentence\n",
    "\n",
    "    def test_sentence_input(self, sentence):\n",
    "        \"\"\"\n",
    "        A basic sentence filter. This one rejects sentences that contain\n",
    "        the type of punctuation that would look strange on its own\n",
    "        in a randomly-generated sentence. \n",
    "        \"\"\"\n",
    "        sentence = sentence.text\n",
    "        reject_pat = re.compile(r\"(^')|('$)|\\s'|'\\s|[\\\"(\\(\\)\\[\\])]\")\n",
    "        # Decode unicode, mainly to normalize fancy quotation marks\n",
    "        if sentence.__class__.__name__ == \"str\":\n",
    "            decoded = sentence\n",
    "        else:\n",
    "            decoded = unidecode(sentence)\n",
    "        # Sentence shouldn't contain problematic characters\n",
    "        if re.search(reject_pat, decoded): return False\n",
    "        return True\n",
    "\n",
    "    def generate_corpus(self, text):\n",
    "        \"\"\"\n",
    "        Given a text string, returns a list of lists; that is, a list of\n",
    "        \"sentences,\" each of which is a list of words. Before splitting into \n",
    "        words, the sentences are filtered through `self.test_sentence_input`\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_split(text)\n",
    "        passing = filter(self.test_sentence_input, sentences)\n",
    "        runs = map(self.word_split, sentences)\n",
    "        print(runs[0])\n",
    "        return runs\n",
    "\n",
    "# Generated the model\n",
    "model = TaggedText(input_doc)\n",
    "# A sentence based on the model\n",
    "print(model.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computational</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linguistics</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>-4.329765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an</td>\n",
       "      <td>-5.953294</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interdisciplinary</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>field</td>\n",
       "      <td>-9.710699</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>concerned</td>\n",
       "      <td>-9.861534</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>with</td>\n",
       "      <td>-5.363765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>statistical</td>\n",
       "      <td>-11.639928</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>or</td>\n",
       "      <td>-5.715355</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rule</td>\n",
       "      <td>-9.545105</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.202416</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>based</td>\n",
       "      <td>-8.318480</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>modeling</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>of</td>\n",
       "      <td>-4.128464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>natural</td>\n",
       "      <td>-9.312656</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>language</td>\n",
       "      <td>-8.811478</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>from</td>\n",
       "      <td>-6.028811</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.983075</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>computational</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>perspective</td>\n",
       "      <td>-9.924529</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>as</td>\n",
       "      <td>-5.507394</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>well</td>\n",
       "      <td>-7.117937</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>as</td>\n",
       "      <td>-5.507394</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>study</td>\n",
       "      <td>-9.603720</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>of</td>\n",
       "      <td>-4.128464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>appropriate</td>\n",
       "      <td>-10.032200</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>pictures</td>\n",
       "      <td>-9.748672</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>of</td>\n",
       "      <td>-4.128464</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>a</td>\n",
       "      <td>-3.983075</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>red</td>\n",
       "      <td>-9.453293</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>truck</td>\n",
       "      <td>-10.898046</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>search</td>\n",
       "      <td>-9.428295</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>engine</td>\n",
       "      <td>-10.291152</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>will</td>\n",
       "      <td>-6.105684</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>still</td>\n",
       "      <td>-7.043760</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>find</td>\n",
       "      <td>-7.562676</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>information</td>\n",
       "      <td>-8.817930</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>desired</td>\n",
       "      <td>-11.827469</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>by</td>\n",
       "      <td>-6.114920</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>matching</td>\n",
       "      <td>-11.781653</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>words</td>\n",
       "      <td>-8.599854</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>such</td>\n",
       "      <td>-7.619059</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>as</td>\n",
       "      <td>-5.507394</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>\"</td>\n",
       "      <td>-4.700859</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>four</td>\n",
       "      <td>-9.416868</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>-</td>\n",
       "      <td>-5.202416</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>wheeled</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>\"</td>\n",
       "      <td>-4.700859</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>with</td>\n",
       "      <td>-5.363765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>\"</td>\n",
       "      <td>-4.700859</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>car\".[37</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>]</td>\n",
       "      <td>-5.498423</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>-4.458347</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3756 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  log_probability stop? punctuation? whitespace?  \\\n",
       "0         Computational       -19.579313                                  \n",
       "1           linguistics       -19.579313                                  \n",
       "2                    is        -4.329765   Yes                            \n",
       "3                    an        -5.953294   Yes                            \n",
       "4     interdisciplinary       -19.579313                                  \n",
       "5                 field        -9.710699                                  \n",
       "6             concerned        -9.861534                                  \n",
       "7                  with        -5.363765   Yes                            \n",
       "8                   the        -3.425446   Yes                            \n",
       "9           statistical       -11.639928                                  \n",
       "10                   or        -5.715355   Yes                            \n",
       "11                 rule        -9.545105                                  \n",
       "12                    -        -5.202416                Yes               \n",
       "13                based        -8.318480                                  \n",
       "14             modeling       -19.579313                                  \n",
       "15                   of        -4.128464   Yes                            \n",
       "16              natural        -9.312656                                  \n",
       "17             language        -8.811478                                  \n",
       "18                 from        -6.028811   Yes                            \n",
       "19                    a        -3.983075   Yes                            \n",
       "20        computational       -19.579313                                  \n",
       "21          perspective        -9.924529                                  \n",
       "22                    ,        -3.391480                Yes               \n",
       "23                   as        -5.507394   Yes                            \n",
       "24                 well        -7.117937   Yes                            \n",
       "25                   as        -5.507394   Yes                            \n",
       "26                  the        -3.425446   Yes                            \n",
       "27                study        -9.603720                                  \n",
       "28                   of        -4.128464   Yes                            \n",
       "29          appropriate       -10.032200                                  \n",
       "...                 ...              ...   ...          ...         ...   \n",
       "3726           pictures        -9.748672                                  \n",
       "3727                 of        -4.128464   Yes                            \n",
       "3728                  a        -3.983075   Yes                            \n",
       "3729                red        -9.453293                                  \n",
       "3730              truck       -10.898046                                  \n",
       "3731                  ,        -3.391480                Yes               \n",
       "3732                the        -3.425446   Yes                            \n",
       "3733             search        -9.428295                                  \n",
       "3734             engine       -10.291152                                  \n",
       "3735               will        -6.105684   Yes                            \n",
       "3736              still        -7.043760   Yes                            \n",
       "3737               find        -7.562676                                  \n",
       "3738                the        -3.425446   Yes                            \n",
       "3739        information        -8.817930                                  \n",
       "3740            desired       -11.827469                                  \n",
       "3741                 by        -6.114920   Yes                            \n",
       "3742           matching       -11.781653                                  \n",
       "3743              words        -8.599854                                  \n",
       "3744               such        -7.619059   Yes                            \n",
       "3745                 as        -5.507394   Yes                            \n",
       "3746                  \"        -4.700859                Yes               \n",
       "3747               four        -9.416868   Yes                            \n",
       "3748                  -        -5.202416                Yes               \n",
       "3749            wheeled       -19.579313                                  \n",
       "3750                  \"        -4.700859                Yes               \n",
       "3751               with        -5.363765   Yes                            \n",
       "3752                  \"        -4.700859                Yes               \n",
       "3753           car\".[37       -19.579313                                  \n",
       "3754                  ]        -5.498423                Yes               \n",
       "3755               \\n\\n        -4.458347                            Yes   \n",
       "\n",
       "     number? out of vocab.?  \n",
       "0                            \n",
       "1                            \n",
       "2                            \n",
       "3                            \n",
       "4                            \n",
       "5                            \n",
       "6                            \n",
       "7                            \n",
       "8                            \n",
       "9                            \n",
       "10                           \n",
       "11                           \n",
       "12                           \n",
       "13                           \n",
       "14                           \n",
       "15                           \n",
       "16                           \n",
       "17                           \n",
       "18                           \n",
       "19                           \n",
       "20                           \n",
       "21                           \n",
       "22                           \n",
       "23                           \n",
       "24                           \n",
       "25                           \n",
       "26                           \n",
       "27                           \n",
       "28                           \n",
       "29                           \n",
       "...      ...            ...  \n",
       "3726                         \n",
       "3727                         \n",
       "3728                         \n",
       "3729                         \n",
       "3730                         \n",
       "3731                         \n",
       "3732                         \n",
       "3733                         \n",
       "3734                         \n",
       "3735                         \n",
       "3736                         \n",
       "3737                         \n",
       "3738                         \n",
       "3739                         \n",
       "3740                         \n",
       "3741                         \n",
       "3742                         \n",
       "3743                         \n",
       "3744                         \n",
       "3745                         \n",
       "3746                         \n",
       "3747     Yes                 \n",
       "3748                         \n",
       "3749                         \n",
       "3750                         \n",
       "3751                         \n",
       "3752                         \n",
       "3753                    Yes  \n",
       "3754                         \n",
       "3755                         \n",
       "\n",
       "[3756 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in doc]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE Today\n",
      "ORG \n",
      "\n",
      "The Association for Computational Linguistics\n",
      "WORK_OF_ART \n",
      "\n",
      "...\n",
      "CARDINAL 1\n",
      "CARDINAL 2\n",
      "CARDINAL 2.1\n",
      "CARDINAL 2.2\n",
      "CARDINAL 2.3\n",
      "CARDINAL 2.3.1\n",
      "CARDINAL 2.3.2\n",
      "CARDINAL \n",
      "2.4\n",
      "CARDINAL 3\n",
      "CARDINAL 4\n",
      "CARDINAL 5\n",
      "CARDINAL 6\n",
      "PERCENT 7\tReferences\n",
      "PERCENT 8\tExternal\n",
      "GPE the United States\n",
      "DATE the 1950s\n",
      "NORP Russian\n",
      "DATE the 1960s\n",
      "CARDINAL one\n",
      "CARDINAL one\n",
      "CARDINAL one\n",
      "CARDINAL four\n",
      "CARDINAL one\n",
      "ORG Crucially\n",
      "GPE Pólya\n",
      "DATE modern-day\n",
      "LANGUAGE English\n",
      "CARDINAL One\n",
      "CARDINAL One\n",
      "NORP English\n",
      "ORG IBM\n",
      "MONEY over 4.5 million words\n",
      "NORP American\n",
      "LANGUAGE English\n",
      "CARDINAL One\n",
      "CARDINAL two\n",
      "DATE early in the lifetime of a field of study\n",
      "LANGUAGE English\n",
      "NORP Japanese\n",
      "NORP Japanese\n",
      "NORP Japanese\n",
      "DATE October 2015\n",
      "CARDINAL only half\n",
      "PERSON Alan Turing\n",
      "ORG the Turing Test\n",
      "DATE 1950\n",
      "PERSON Alan Turing\n",
      "DATE one day\n",
      "CARDINAL two\n",
      "CARDINAL one\n",
      "GPE Turing\n",
      "DATE Today\n",
      "GPE Turing\n",
      "PERSON Joseph Weizenbaum\n",
      "ORG MIT\n",
      "ORG ELIZA\n",
      "CARDINAL One\n",
      "ORG ELIZA\n",
      "PERSON Joseph Weizenbaum\n",
      "ORG MIT\n",
      "DATE 1966\n",
      "NORP Rogerian\n",
      "ORG ELIZA\n",
      "ORG ELIZA\n",
      "ORG ELIZA\n",
      "ORDINAL first\n",
      "ORDINAL first\n",
      "PERSON Markov\n",
      "WORK_OF_ART translation.[25] The\n",
      "NORP German\n",
      "NORP French\n",
      "ORDINAL first\n",
      "CARDINAL five\n",
      "ORG ELIZA\n",
      "PERSON Siri\n",
      "NORP Bayesian\n",
      "PERSON Bledsoe\n",
      "GPE Browing\n",
      "DATE 1959\n",
      "CARDINAL one\n",
      "NORP Bayesian\n",
      "PERSON Mosteller\n",
      "PERSON Wallace\n",
      "DATE 1963\n",
      "ORG The Federalist Papers\n",
      "PERSON Madison\n",
      "DATE 1971\n",
      "PERSON Terry Winograd\n",
      "ORG SHRDLU\n",
      "ORG SHRDLU\n",
      "ORG NASA\n",
      "ORG LUNAR\n",
      "ORG Apollo\n",
      "DATE the 1960s and 1970s\n",
      "GPE Markov\n",
      "ORG Rabiner\n",
      "CARDINAL 1989.[32\n",
      "DATE the late 70s\n",
      "ORG IBM\n",
      "NORP Bayesian\n",
      "ORG Apple\n",
      "ORG Siri\n",
      "ORG Google Translate\n",
      "NORP Social\n",
      "PERSON Twitter\n",
      "CARDINAL four\n",
      "CARDINAL four\n"
     ]
    }
   ],
   "source": [
    "for ent in nlp(input_doc).ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
